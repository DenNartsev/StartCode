Задача: научиться по нескольким словам произведения "Дом, который построил Джек" предсказывать следующее слово.

Решение: позволим нейронной сети найти неизвестную зависимость между контекстом и словом. Создаем словарь всех слов текста и кодируем их при помощи One hot encode
Контекст - сумма one hot представлений слов, соседних с искомым. Рассматриваем задачу, как задачу классификации следующего слова, где классы - слова. Соответственно функцией потерь будет Перекрестная Энтропия.
Таким образом обучаем несколько сетей, каждая из которых предсказывает слово по некоторому количеству(size) слов до и (next_size) слов после. 
Из этих сетей берем лучшую по функции потерь, а также лучшуу для которой next_size = 0 (то есть она смотрит только на предыдущие слова)
Приложеный график - процесс обучения самой эффективной из полученных сетей, то есть зависимость loss от номера итерации.
