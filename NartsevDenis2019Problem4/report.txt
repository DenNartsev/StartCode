Условие: В тексте ("Вот дом который построил Джек") необходимо по нескольким предыдущим словам предсказывать следующее. Проанализировать ошибку.

Решение: код находится в файле main.ipynb. Создаем словарь из слов текста, каждому слову соответсвует некоторый номер в словаре. Кодируем слова при помощи One hote encode.
Предпологаем, что существует неизвестная зависимость слова от контекста(в частности от нескольких предыдущих слов). Рассматриваем задачу, как задачу классификации, где классы - слова в словаре.
Ищем зависимость с помощью нейросети, ошибку классификации определяем перекресной энтропией. На вход подаем сумму One Hot представлений слов из контекста, на выходе после softmax получаем распределение вероятности принадлежности к тому или иному классу.

В коде оценены точности предсказаний нейросетей обученных на разных контекстах, то есть при их обучении менялось количество слов до и после предсказываемого. Показаны попытки восстановить текст по контексту при помощи лучшей из опробованных нейросетей и при помощи лучшей из тех, что использует только предыдущие слова.
Прилагаемый график показывает процесс обучения лучшей найденной нейросети.
Можно видеть, что при обучении на большом числе предшествующих слов точность результата ухудшается. Причина: таким сети необходимо больше времени на нахождение зависимости, к тому же велик шанс ошибки на последующих словах, так как сумма one hot представлений слов из их контекста во многих позициях совпадает.
Также больше точность у сетей, обученных с использованием следующих слов вместе с предыдущими. Причина: структура текста. Многие части текста повторяются, потому информации о предыдущих словах бывает недостаточно. Это хорошо видно на полученном в коде результате, где сеть предсказывает несколько равновероятных вариантов, но не имеет возможности определить верный.
